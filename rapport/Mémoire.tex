% !TeX spellcheck = fr_FR
\documentclass[a4paper]{article} 

\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{url}
\usepackage[toc,page]{appendix}
\usepackage{bm}
\renewcommand{\appendixpagename}{Annexes}
\renewcommand{\appendixtocname}{Annexes}

\author{Raphael Palmeri\\1 ère Master en Sciences Informatiques\\ 2020-2021}
\begin{document}

\begin{titlepage}
	\centering
	{\scshape\LARGE Université de Mons \par}
	\vspace{1cm}
	{\scshape\large Faculté des Sciences \par}
	\vspace{1.5cm}
	{\huge\bfseries Le phénomène de double descente au sein de l'Apprentissage Automatique \par}
	\vspace{2cm}
	{\Large\itshape Raphael Palmeri \par}
	\vspace{2.0cm}
	{Sous la direction de Souhaib Ben Taieb \par}
	\vspace{1.0cm}
	{1 ère Master en Sciences informatiques \par}
	\vspace{0.5cm}
	{Je soussigné, Palmeri Raphael, atteste avoir respecté les règles éthiques en vigueur \par}
	\vspace{4cm}
	Année universitaire 2020-2021
	\vfill
	\includegraphics[scale=0.3]{"Faculte_Sciences_logo".png}
	\hfill
	\includegraphics[scale=0.4]{"UMONS-Logo".jpg}
\end{titlepage}
\newpage
\thispagestyle{empty}
\mbox{}
\newpage

\tableofcontents
\newpage

\section{Introduction}
Dans le cadre de notre Master en Sciences Informatiques en horaire décalé, il nous est demandé de réaliser un mémoire sur un sujet proposé par les enseignants de l'UMons de la section Sciences Informatiques. Mon choix de sujet s'est porté sur "Le phénomène de double descente au sein de l'Apprentissage Machine (\textit{'The double descent phenomenon in machine learning'})".\newline

Dans un premier temps, je vais présenter ce qu'est l'apprentissage Machine ainsi qu'expliquer brièvement les différentes sous-catégories existantes au sein de celui-ci. \newline

Dans une deuxième partie, je vais expliquer les notions de biais et de variance, ainsi que le phénomène de compromis entre ces deux notions au sein de l'apprentissage machine. \newline

Dans la troisième partie, je vais démontrer mathématiquement le compromis biais-variance et expliquer cette démonstration plus en détails. \newline

Dans la quatrième partie, à l'aide de simulations, nous montrerons l'effet de ce phénomène ainsi que son comportement en influençant sur différents paramètres de la simulation.\newline

\newpage

\section{Apprentissage automatique ("Machine Learning")}

L'apprentissage automatique, c'est la capacité d'un ordinateur à "apprendre" en se basant sur des données mises à sa disposition. Le terme "apprendre" dans ce cas-ci désigne la capacité à détecter/trouver des répétitions (\textit{'patterns'}) dans ces données. Ces répétitions permettront ensuite à la machine de donner une expertise par rapport à un problème donné ou une réponse à une certaine question. \cite{UnderstandingML}\cite{MLPracticalApproach} \newline 

L'apprentissage automatique est notamment utilisé dans différents domaines tels que le domaine de l'automobile avec ses voitures sans conducteurs, le domaine des Finances afin de notamment détecter les fraudes mais aussi le domaine de la santé avec la possibilité d'essayer de produire un diagnostic sur base des informations disponibles à propos d'un patient, ... \newline

Afin de réaliser de l'apprentissage automatique, il est nécessaire d'avoir deux choses: un algorithme d'apprentissage (voir sous-section \ref{LearningAlgo}) et des données (voir sous-section \ref{Data}).

\newpage

\subsection{Algorithme d'apprentissage}
\label{LearningAlgo}
Il existe bien des algorithmes différents utilisés en machine learning. il existe différents types d'algorithme d'apprentissage:

\subsubsection{Apprentissage supervisés}
Dans ce type d'apprentissage, la machine reçoit un ensemble de données avec les classes de tout les exemples existants.
Par exemple, un expert aura déjà défini les différentes classes possibles pour la reconnaissance d'objets via des images ('Chaise', 'Table', 'Chien', 'Chat', ...). Les algorithmes de cette famille vont dès lors se basé sur ces classes déjà définies afin de pouvoir attribuer une classe (que l'on espère correcte) à une nouvelle donnée encore inconnue. Dans l'exemple ci-dessus, on parlera de \textbf{"Classification"}. \newline
Il existe aussi des problèmes de \textbf{"Régression"}, ceux-ci tentent de liés une nouvelle donnée à un nombre réel. Par exemple, dans le cadre de l'estimation de prix de maison. \newline

Étant donné le fait que le compromis Biais-Variance (voir section \ref{B-V}) n'existent qu'au sein de cette famille d'algorithme, il est logique que celle-ci soit la famille que nous étudierons le plus dans ce rapport.

\subsubsection{Apprentissage non-supervisés}
Dans le cas d'algorithme non-supervisé, les données d'entrainement et de test sont mélangés. Le modèle n'aura aucun exemple pour s'aider à détecter un pattern, il devra le faire par lui même en étudiant les similarités entre les différentes données et les ranger par groupes afin qu'un expert puisse les utiliser dans le cadre de recherche par exemple. Dans le cadre de l'utilisation de cette famille, on parlera de \textbf{"Clustering"}. \newline

\subsubsection{Apprentissage semi-supervisés et Apprentissage actif}
Dans la plupart des situations, il est impossible de classifié l'ensemble des données d'apprentissage. Dans ce genre de cas, la machine doit dès lors apprendre des classes qui lui sont fournies mais aussi des données non labellisées, c'est ce que l'on appelle l'apprentissage semi-supervisé. Dans le cadre où ce n'est pas un expert qui donne les classes mais bien la machine qui tente de les labellisées, on se trouve dans le cas de l'apprentissage actif. 

\subsubsection{Apprentissage par transfert et apprentissage multitâche}
L'idée principale derrière l'apprentissage par transfert est d'aider le modèle à s'adapter à des situations qu'il n'as pas rencontrés précédemment. Cette forme d'apprentissage s'appuie sur le fait de tuner un modèle générale pour lui permettre de travailler dans un nouveau domaine.

\newpage

\subsubsection{Apprentissage par renforcement}
L'apprentissage par renforcement se base sur l'idée de maximisé une récompense selon une ou plusieurs actions. On va dés lors définir en fonction des actions, si elles sont encouragées ou au contraire, découragés.

\subsubsection{Exemples d'algorithmes}

\begin{itemize}
	\item Prédicteurs linéaires (\textit{'Linear Predictors'}) : tels que la régression linéaire, perceptron ...
	\item Boosting
	\item Support Vector Machines
	\item Arbres de décision (\textit{'Decision Trees'})
	\item Voisin le plus proche (\textit{'Nearest Neighbor'})
	\item Réseau de neurones (\textit{'Neural Networks'})
	\item ...
\end{itemize}

\newpage

\subsection{Notation mathématiques}
Dans cette sous-section, je fais expliquer les différentes notations mathématiques nécessaires à la bonne compréhension des prochains chapitres.

Dans le cadre d'un apprentissage automatique supervisé, on cherche à prédire un résultat $y\in \bm{Y}$ à partir d'une donnée $x \in \bm{X}$ où les paires $(x,y)$ proviennent d'une distribution inconnue $\bm{D}$. \newline

Le problème d'apprentissage automatique consiste à apprendre une fonction $f' : \bm{X} \to \bm{Y}$ à partir d'un ensemble de données d'entrainement fini $\bm{S}$ contenant $m$ variables indépendantes et identiquement distribuées (\textit{i.i.d}) provenant de $\bm{D}$. \newline

$f'$ peut aussi être vue comme étant une hypothèse $h \in \bm{H}$, choisie à partir d'une classe d'hypothèses $\bm{H$} contenant des fonctions possibles pour le modèle. \newline

Dans un cadre idéal, la fonction $f'$ serait équivalente à la fonction $f$, la 'vrai' fonction qui régit $\bm{X} \to \bm{Y}$.

\subsubsection{La fonction de perte quadratique}
Elle s'exprime comme suit :

\[ (y - y')^2 \]

où \textbf{y} représente la valeur véritable de la vrai fonction et \textbf{y'} représente la valeur estimée par le modèle. \newline

L'erreur quadratique moyenne quant à elle n'est que la moyenne des erreurs sur l'ensemble des données :

\[ MSE = \frac{1}{n} \sum_{(x,y)\in D} (y - y')^2 \]

\subsubsection{Espérance mathématique d'une variable aléatoire}
Dans les preuves de la décomposition du biais-variance, on peut y trouver une notation statistique appelée l'espérance mathématique qui se note $ E(x) $ pour une variable aléatoire $x$. \newline

L'espérance mathématique représente la moyenne pondérée des valeurs que peut prendre cette variable. 

\newpage

\newpage

\subsection{Données}
\label{Data}
Afin de permettre le bon fonctionnement de l'apprentissage automatique, il est nécessaire d'avoir des données. Celles-ci doivent être présentes en quantité et, dans le meilleur des cas, elles doivent êtres "nettoyées" c-à-d, il faut parfois retirer des attributs inutiles, en modifier certains pour qu'il soit compréhensibles pour l'algorithme et certains sont inutilisables car incomplets. \newline

Ces données peuvent être distinguées en 2 catégories:

\subsubsection{Données d'apprentissage}
Ces données sont des exemples déjà traitées par un expert dans le domaine qui peuvent être utilisés comme exemple d'apprentissage pour les algorithmes supervisés. Grâce à celles-ci, l'algorithme pourra générer un modèle qui pourra estimer la valeur (ou la classe ) en fonction d'une donnée inconnue.

\subsubsection{Données de test}
Ces données sont destinés à valider le modèle crée par l'algorithme d'apprentissage. l'idée est de fournir des données non vues précédemment à la machine afin de vérifier et valider son comportement. Si le modèle produit des résultats extrêmement éloignés de la vérité, c'est qu'il n'est pas encore prêt. Il faut donc repasser par une phase d'apprentissage en fournissant potentiellement plus de données d'apprentissage et/ou en les rendant plus précises afin que la machine établissent un nouveau modèle dont les réponses seront plus correctes.

\newpage

\section{Le compromis Biais-Variance}
\label{B-V}

Afin de mieux comprendre la notion du compromis de Biais-Variance, il est important de comprendre sa version plus général qui est le compromis Approximation-Estimation (voir sous-section \ref{A-E})

\subsection{Le compromis approximation-estimation}
\label{A-E}
L'erreur totale au sein de l'apprentissage machine est constitué de 3 choses: \newline

\[ error_{total} = error_{generalization} + error_{training} + error_{irreductible} \]

1. l'erreur de généralisation (\textit{'generalization error'}): cette erreur est la conséquence de la sélection d'un sous-ensemble que l'on considère comme étant représentatif. De part cette sélection, on induit une possible erreur.\newline

2. l'erreur d'entrainement (\textit{'training error'}) : cette erreur est la conséquence de l'apprentissage, dans nos données sélectionnées pour l'apprentissage de la machine, on peut avoir des cas spécifiques qui ne se présentent que dans notre ensemble d'apprentissage. ce qui mènera le modèle à un 'biais d'apprentissage' et peut diminuer la précision de celui-ci lors de l'utilisation de données de test.\newline

3. l'erreur irréductible (\textit{'irreductible error'}): cette erreur est la conséquence d'un traitement peu efficace des données en amont de l'apprentissage, les données qui seront utilisées par l'algorithme doivent êtres nettoyés avant d'être utilisées. cette erreur ne dépends donc pas de l'algorithme directement.\newline
 \textit{"Garbage In, Garbage Out"}, ce qui signifie que si les données en entrée ne sont pas correctes, les résultats ne sauraient l'être. \newline

\newpage

\subsection{Le compromis Biais-Variance}

Le compromis biais-variance permet de quantifier le compromis approximation-estimation lorsque l'on utilise la fonction de perte quadratique (ou perte \textit{L2}) et plus particulièrement, l'erreur quadratique moyenne (\textit{MSE}).

\subsubsection{Le biais}
Le biais est la mesure qui montre à quel point le modèle établi par l'algorithme d'apprentissage supervisé est proche de la 'vrai' fonction d'un problème donné. \newline

La figure \ref{BiasRepresentation} montre une représentation graphique du biais. \textit{f} représente la 'vrai' fonction d'un problème donné, \textit{H} représente l'ensemble des hypothèses choisies et le point noir représente une hypothèse sélectionnée parmi \textit{H}. \newline

\begin{figure}[!h]
	\centering
	\includegraphics[scale=1]{"Representation Biais".png}
	\caption{Représentation du Biais.}
	\cite{BiasVarianceTradeoffTextbooksUpdate}
	\label{BiasRepresentation}
\end{figure}

\subsubsection{La Variance}

La variance est la variation entre la valeur d'un ensemble de données de test par rapport à la valeur donnée par le modèle choisi. 

\newpage

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.75]{"BiasVarianceTradeoff".png}
	\caption{Représentation de l'erreur en fonction de la complexité du modèle}
	\cite{BiasVarianceTradeoffTextbooksUpdate}
	\label{TradeoffRepresentation}
\end{figure}

La figure \ref{TradeoffRepresentation} montre l'impact du biais et de la variance sur l'erreur d'un modèle et ce en fonction de sa complexité. Elle montre aussi très bien le point d'optimisation de la complexité du modèle lié au compromis entre le biais et la variance.

\newpage

Un exemple concret du compromis biais-variance est celui du tir sur cible :

\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.5]{"exemple Bias-Variance".png}
	\caption{Exemple concret du Compromis Biais-Variance.} \cite{UnderstandingBiasVarianceTradeoff}
	\label{ConcreteExample}
\end{figure}

Dans la figure \ref{ConcreteExample}, on a quatre cibles selon deux axes différents, le biais et la variance, chacun de ces axes peut-être soit faible, soit élevé. \par

Le premier cas (biais faible et variance faible) représente un excellent tireur, il vise toujours le centre et ses tirs sont fortement groupés. \par

Le second cas (biais faible et variance élevée) représente un 'bon' tireur, il vise le centre mais ses tirs sont assez dispersés. \par

Le troisième cas (biais élevé et variance faible) représente un tireur moyen, il ne vise pas le centre mais n'est pas non plus hors de la cible ou au bord de celle-ci et ses tirs sont fortement groupés. \par

Le quatrième cas (biais élevé et variance élevée) représente un mauvais tireur, il ne vise pas le centre et ses tirs sont très dispersés. \par

\newpage

\subsection{Décomposition du biais-variance}
\label{decomposition_Biais_Variance_section}

Considérons le modèle suivant : 
\begin{equation}
\label{decomposition_Biais_Variance}
y = f(x) + \epsilon
\end{equation}

où : 

\begin{itemize}
	\item x $\sim$ p(x)
	\item f est une fonction fixée inconnue
	\item $\epsilon$ est du bruit aléatoire tel que :
	\begin{itemize}
		\item $E[\epsilon|x] = 0$
		\item $Var(\epsilon|x) = \sigma^2$
	\end{itemize}
\end{itemize}


étant donné un set de données $D = {(x_i, y_i)}^n_{i=1}$ où $(x_i, y_i)$ est un échantillon provenant de (1), et un ensemble d'hypothèses $H$, on calcule : \newline
\[ g^{(D)} = argmin_{h\in H}  E_{in}(h) := \frac{1}{n} \sum_{i=1}^{n} L(y_i,h(x_i)) \]

étant donné D, l'erreur au carré hors-échantillon de $g^{(D)}$ est : \newline
\[ E_{out}(g^{(D)}) = E_{x,y}[(y - g^{(D)}(x))^2] \]

considérons 
\begin{equation}
E_D[E_{out}(g^{(D)})] = E_{x,y,D}[(y - g^{(D)}(x))^2]
\end{equation}
représentant l'espérance moyenne sur les variables x, y et D. \newline

En prenant $ \bar{g} = E_D[g^{(D)}(x)]$ , on peut décomposer (2) comme suit 
\[ \underbrace{E_x[(f(x) - \bar{g}(x))^2]}_{Biais} + \underbrace{E_{x,D}[(\bar{g}(x) - g^{(D)}(x))^2]}_{Variance} + \underbrace{\sigma^2}_{Variance irreductible} \]

\newpage

On sait que le compromis biais-variance s'exprime comme suit : 
\[ E_{x,y,D}[(y-g^{(D)}(x))^2] \]
En sachant que le g moyen est :
\begin{equation}
\label{g_moyen}
\bar{g}(x) = E_D [g^{(D)}(x)]
\end{equation}
Prouvons qu'il est égal à :
\[ E_x[(f(x) - \bar{g}(x))^2] + E_{x,D}[(\bar{g}(x) - g^{(D)}(x))^2] + \sigma^2 \]

\section{Simulation}
 
\newpage

\section{Conclusion}

\newpage

\begin{appendices}
	
	\section{Preuve mathématique}
	
	Reprenons l'expression du compromis :
	\begin{equation}
		\label{Bias_Variance_formula}
		E_{x,y,D}[(y-g^{(D)}(x))^2]
	\end{equation}
	
	On peut exprimer (\ref{Bias_Variance_formula}) comme suit : 
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_1}
		E_x [E_{y,D} [(y - g^{(D)}(x))^2 | x]]
	\end{equation}
	
	En fixant $x$, on peut simplifier (\ref{Bias-Variance_formula_proof_1}) :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_2}
		E_{y,D}[ (y - g^{(D)}(x))^2]
	\end{equation}
	
	En ajoutant $ - f(x) + f(x) $ à l'équation \ref{Bias-Variance_formula_proof_2}, on obtient :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_3}
		E_{y,D}[ (y - f(x) + f(x) - g^{(D)}(x))^2]
	\end{equation}
	
	En considérant $ y - f(x) $ comme étant $a$ et $ f(x) - g^{(D)}(x)$ comme $b$ et en appliquant la formule $(a+b)^2 = a^2 + b^2 + 2ab$ dans l'équation (\ref{Bias-Variance_formula_proof_3}), on obtient :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_4}
		E_{y,D} [(y-f(x))^2] + E_{y,D} [(f(x) - g^{(D)}(x))^2] + 2E_{y,D} [(y-f(x)) (f(x) - g^{(D)}(x)) ]
	\end{equation}
	
	En utilisant (\ref{decomposition_Biais_Variance}) pour remplacer $y$ dans l'équation (\ref{Bias-Variance_formula_proof_4}), on obtient :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_5}
		E_{y,D} [(f(x) + \epsilon - f(x))^2] + E_{y,D} [(f(x) - g^{(D)}(x))^2] + 2E_{y,D} [(y-f(x)) (f(x) - g^{(D)}(x)) ]
	\end{equation}
	
	En simplifiant l'équation (\ref{Bias-Variance_formula_proof_5}), on obtient :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_6}
		E_{y,D} [(\epsilon)^2] + E_{y,D} [(f(x) - g^{(D)}(x))^2] + 2E_{y,D} [(y-f(x)) (f(x) - g^{(D)}(x)) ]
	\end{equation}
	
	En utilisant la définition de la Variance de $\epsilon$ de la section \ref{decomposition_Biais_Variance_section}, on peut simplifier l'équation (\ref{Bias-Variance_formula_proof_6}) comme suit :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_7}
		\sigma^2 + E_{y,D} [(f(x) - g^{(D)}(x))^2] + 2E_{y,D} [(y-f(x)) (f(x) - g^{(D)}(x)) ]
	\end{equation}
	
	En utilisant (\ref{intermediate_proof_a_9}) (voir section \ref{intermediate_proof_a_subsection}) dans l'équation (\ref{Bias-Variance_formula_proof_7}), on obtient :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_8}
		\sigma^2 + E_{y,D} [(f(x) - g^{(D)}(x))^2] + 0
	\end{equation}
	
	En ajoutant $ -\bar{g}(x) + \bar{g}(x)$ à l'équation (\ref{Bias-Variance_formula_proof_8}) dans le terme $ E_{y,D} [(f(x) - g^{(D)}(x))^2]$, on obtient :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_9}
		\sigma^2 + E_{y,D} [(f(x) -\bar{g}(x) + \bar{g}(x) - g^{(D)}(x))^2] + 0
	\end{equation}
	
	En considérant $ f(x) -\bar{g}(x) $ comme étant $a$ et $ \bar{g}(x) - g^{(D)}(x)$ comme $b$ et en appliquant la formule $(a+b)^2 = a^2 + b^2 + 2ab$ dans l'équation (\ref{Bias-Variance_formula_proof_9}), on obtient :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_10}
		\sigma^2 + E_{y,D} [(f(x) -\bar{g}(x))^2] + E_{y,D} [(\bar{g}(x) - g^{(D)}(x))^2] + 2 E_{y,D} [(f(x) -\bar{g}(x)) (\bar{g}(x) - g^(D)(x))]
	\end{equation}
	
	En vérifiant les espérances, on peut encore simplifier l'équation (\ref{Bias-Variance_formula_proof_10}) en :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_11}
		\sigma^2 + (f(x) -\bar{g}(x))^2 + E_{D} [(\bar{g}(x) - g^{(D)}(x))^2] + 2 E_{y,D} [(f(x) -\bar{g}(x)) (\bar{g}(x) - g^(D)(x))]
	\end{equation}
	
	En utilisant la preuve intermédiaire (\ref{intermediate_proof_b_5}) (voir section \ref{intermediate_proof_b_subsection}), on obtient l'équation suivante :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_12}
		\sigma^2 + (f(x) -\bar{g}(x))^2 + E_{D} [(\bar{g}(x) - g^{(D)}(x))^2] + 0
	\end{equation}
	
	et finalement en ré-appliquant l'espérance de x que nous avions retiré pour faciliter la notation, on obtient :
	
	\begin{equation}
		\label{Bias-Variance_formula_proof_13}
		\sigma^2 + E_x[(f(x) -\bar{g}(x))^2] + E_{x,D} [(\bar{g}(x) - g^{(D)}(x))^2]
	\end{equation}
	
	ce qui prouve bien que $ E_{x,y,D}[(y-g^{(D)}(x))^2] $ est équivalent à (\ref{Bias-Variance_formula_proof_13})
	\newpage
	
	\section{Preuve intermédiaire de $2E_{y,D} [(y-f(x)) (f(x) - g^{(D)}(x)) ]$}
	\label{intermediate_proof_a_subsection}
	Prouvons que $E_{y,D} [(y-f(x)) (f(x) - g^{(D)}(x)) ]$ est $= 0$ 
	
	\begin{equation}
		\label{intermediate_proof_a_1}
		E_{y,D} [(y-f(x)) (f(x) - g^{(D)}(x)) ]
	\end{equation}
	
	On peut distribuer dans l'équation (\ref{intermediate_proof_a_1}), on obtient :
	
	\begin{equation}
		\label{intermediate_proof_a_2}
		E_{y,D} [yf(x) -yg^{(D)}(x) - f^2(x) + f(x)g^{(D)}(x)]
	\end{equation}
	
	En utilisant (\ref{decomposition_Biais_Variance}) dans l'équation (\ref{intermediate_proof_a_2}), on obtient :
	
	\begin{equation}
		\label{intermediate_proof_a_3}
		E_{y,D} [(f(x) + \epsilon)f(x) -(f(x) + \epsilon)g^{(D)}(x) - f^2(x) + f(x)g^{(D)}(x)]
	\end{equation}
	
	En séparant les différents éléments et en simplifiant dans (\ref{intermediate_proof_a_3}), on obtient :
	
	\begin{equation}
		\label{intermediate_proof_a_4}
		E_{y,D} [f^2(x) + \epsilon f(x)] - E_{y,D}[f(x)g^{(D)}(x) + \epsilon g^{(D)}(x)] - E_{y,D}[f^2(x)] + E_{y,D}[f(x)g^{(D)}(x)]
	\end{equation}
	
	En vérifiant les espérances, on peut encore simplifier l'équation (\ref{intermediate_proof_a_4}) en :
	
	\begin{equation}
		\label{intermediate_proof_a_5}
		f^2(x) + \epsilon f(x) - E_D[f(x)g^{(D)}(x) + \epsilon g^{(D)}(x)] - f^2(x) + E_D[ f(x)g^{(D)}(x)]
	\end{equation}
	
	En utilisant (\ref{g_moyen}) dans l'équation (\ref{intermediate_proof_a_5}), on obtient :
	
	\begin{equation}
		\label{intermediate_proof_a_6}
		f^2(x) + \epsilon f(x) - f(x)\bar{g}(x) + \epsilon g^{(D)}(x) - f^2(x) + f(x)\bar{g}(x)
	\end{equation}
	
	Pour faciliter la notation, nous avions fixé $x$, l'équation (\ref{intermediate_proof_a_6})  donne en réalité :
	
	\begin{equation}
		\label{intermediate_proof_a_7}
		E_x[f^2(x) + \epsilon f(x) - f(x)\bar{g}(x) + \epsilon g^{(D)}(x) - f^2(x) + f(x)\bar{g}(x)]
	\end{equation}
	
	En utilisant la définition de l'espérance de $\epsilon$ dans l'équation (\ref{intermediate_proof_a_7}), on obtient :
	
	\begin{equation}
		\label{intermediate_proof_a_8}
		E_x[f^2(x) + 0 f(x) - f(x)\bar{g}(x) + 0 g^{(D)}(x) - f^2(x) + f(x)\bar{g}(x)]
	\end{equation}
	
	En simplifiant l'équation (\ref{intermediate_proof_a_8}), on obtient finalement :
	
	\begin{equation}
		\label{intermediate_proof_a_9}
		E_x[f^2(x) - f(x)\bar{g}(x) - f^2(x) + f(x)\bar{g}(x)] = E_x[0] = 0
	\end{equation}
	
	On a donc prouvé mathématiquement que (\ref{intermediate_proof_a_1}) est bien égale à 0
	\newpage
	
	\section{Preuve intermédiaire de $2E_{y,D} [(f(x)-\bar{g}(x)) (\bar{g}(x) - g^{(D)}(x)) ]$}
	\label{intermediate_proof_b_subsection}
	Prouvons que $E_{y,D} [(f(x)-\bar{g}(x)) (\bar{g}(x) - g^{(D)}(x)) ]$ est $= 0$ 
	
	\begin{equation}
		\label{intermediate_proof_b_1}
		E_{y,D} [(f(x)-\bar{g}(x)) (\bar{g}(x) - g^{(D)}(x)) ]
	\end{equation}
	
	On peut distribuer dans l'équation (\ref{intermediate_proof_b_1}), on obtient :
	
	\begin{equation}
		\label{intermediate_proof_b_2}
		E_{y,D} [ f(x)\bar{g}(x) - f(x)g^{(D)}(x) -g^2(x) + \bar{g}(x)g^{(D)}(x)]
	\end{equation}
	
	en vérifiant les espérances dans l'équation (\ref{intermediate_proof_b_2}), on obtient :
	
	\begin{equation}
		\label{intermediate_proof_b_3}
		f(x)\bar{g}(x) + E_{D} [- f(x)g^{(D)}(x)] + E_{D} [ \bar{g}(x)g^{(D)}(x)] -g^2(x)
	\end{equation}
	
	en appliquant la formule du g moyen (\ref{g_moyen}), on obtient l'équation suivante :
	
	\begin{equation}
		\label{intermediate_proof_b_4}
		f(x)\bar{g}(x) - f(x)\bar{g}(x) + \bar{g^2}(x) -\bar{g^2}(x)
	\end{equation}
	
	Finalement, en simplifiant l'équation (\ref{intermediate_proof_b_4}), on obtient :
	
	\begin{equation}
		\label{intermediate_proof_b_5}
		f(x)\bar{g}(x) - f(x)\bar{g}(x) + \bar{g^2}(x) -\bar{g^2}(x) = 0
	\end{equation}
\end{appendices}

\newpage

\begin{thebibliography}{9}
	
	\bibitem{ReconcilingModernML}
	Belkin M., Hsu D., Ma S., Mandal S.,
	Reconciling modern machine learning practice and the bias-variance trade-off
	\textit{arXiv:1812.11118v2}, November 1-4, 2015, pp. 337-350.
	
	\bibitem{MLPracticalApproach}
	Fernandes de Mello R., Antonelli Ponti M.,
	\textit{Machine Learning A practical Approach on the Statistical Learning Theory},
	Springer, Cham, 2018.
	
	\bibitem{NeuralNetworksBiasVarianceDilemma}
	Geman S., Bienenstock E., Doursat R.,
	Neural Networks and the Bias/Variance Dilemma
	\textit{Neural Computation} 4, 1-58, 1992
	\url{http://direct.mit.edu/neco/article-pdf/4/1/1/812244/neco.1992.4.1.1.pdf}
	
	\bibitem{BiasVarianceTradeoffTextbooksUpdate}
	Neal B.,
	On the Bias-Variance Tradeoff : Textbooks Need an Update
	\textit{arXiv:1912.08286v1}, December 2019
	
	\bibitem{UnderstandingML}
	Shalev-Shwartz S., Ben-David S.,
	\textit{Understanding Machine Learning From Theory to Algorithms},
	Cambridge University Press, 2019 (12th printing).
	
	\bibitem{UnderstandingBiasVarianceTradeoff}
	\url{http://scott.fortmann-roe.com/docs/BiasVariance.html},
	consulté le 18 Juin 2021 à 09:25
	
	
\end{thebibliography}
\newpage

\listoffigures
\newpage


\end{document}